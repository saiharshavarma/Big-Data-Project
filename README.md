# FunnelPulse: Real-Time E-Commerce Funnel Analytics Platform

[![PySpark](https://img.shields.io/badge/PySpark-3.4+-orange.svg)](https://spark.apache.org/)
[![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)](https://www.python.org/)
[![GCP](https://img.shields.io/badge/GCP-Dataproc-4285F4.svg)](https://cloud.google.com/dataproc)

FunnelPulse is an end-to-end big data pipeline for monitoring e-commerce conversion funnels in both **batch** and **real-time streaming** modes. Built using Apache Spark, it implements a **Lakehouse architecture** (Bronze → Silver → Gold) with **statistical anomaly detection** to surface conversion drops and spikes.

**Current Status**: ✅ Fully functional on both local development and Google Cloud Platform (GCP Dataproc)

---

## Table of Contents

1. [Overview](#1-overview)
2. [Architecture](#2-architecture)
3. [Project Structure](#3-project-structure)
4. [Dataset](#4-dataset)
5. [Quick Start](#5-quick-start)
   - [Local Development](#51-local-development)
   - [GCP Deployment](#52-gcp-deployment)
6. [Pipeline Components](#6-pipeline-components)
7. [Running the Pipeline](#7-running-the-pipeline)
8. [Configuration](#8-configuration)
9. [Data Schema](#9-data-schema)
10. [Anomaly Detection](#10-anomaly-detection)
11. [GCP Cost & Resource Management](#11-gcp-cost--resource-management)
12. [Troubleshooting](#12-troubleshooting)
13. [Future Enhancements](#13-future-enhancements)
14. [Kafka Integration (For Production)](#14-kafka-integration-for-production)

---

## 1. Overview

### Problem Statement

E-commerce platforms generate millions of user events daily (views, cart additions, purchases). Understanding the **conversion funnel** and detecting anomalies in real-time is critical for:

- Identifying broken checkout flows
- Detecting fraud or bot activity
- Measuring marketing campaign effectiveness
- Monitoring brand and category performance

### Solution

FunnelPulse provides:

| Feature                     | Description                                                         |
| --------------------------- | ------------------------------------------------------------------- |
| **Batch Analytics**         | Historical funnel metrics by brand, category, and price band        |
| **Streaming Analytics**     | Real-time hourly funnel metrics with watermarking                   |
| **Anomaly Detection**       | Z-score based detection of conversion drops and spikes              |
| **Multi-dimensional Views** | Analyze funnels across brands, categories, time, and price segments |
| **Cloud Ready**             | One-click deployment to GCP Dataproc                                |

### Key Metrics Computed

- **Views** → **Carts** → **Purchases** funnel
- **Conversion rates** at each stage
- **Revenue** by brand, category, time period
- **Anomaly scores** (z-scores) for unusual behavior

---

## 2. Architecture

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         FunnelPulse Architecture                            │
├─────────────────────────────────────────────────────────────────────────────┤
│                                                                             │
│   ┌──────────┐     ┌──────────┐     ┌──────────┐     ┌──────────────────┐  │
│   │ Raw CSV  │────▶│  BRONZE  │────▶│  SILVER  │────▶│      GOLD        │  │
│   │ (Kaggle) │     │ (Parquet)│     │ (Cleaned)│     │   (Aggregated)   │  │
│   └──────────┘     └──────────┘     └──────────┘     └──────────────────┘  │
│        │                                                       │            │
│        │                BATCH PIPELINE                         │            │
│   ═════╪═══════════════════════════════════════════════════════╪══════════  │
│        │              STREAMING PIPELINE                       │            │
│        │                                                       ▼            │
│   ┌──────────┐     ┌──────────┐     ┌──────────┐     ┌──────────────────┐  │
│   │  Stream  │────▶│ Streaming│────▶│ Streaming│────▶│     ANOMALY      │  │
│   │  Input   │     │  Silver  │     │   Gold   │     │    DETECTION     │  │
│   └──────────┘     └──────────┘     └──────────┘     └──────────────────┘  │
│                                                                             │
│   Storage: Local (Parquet) │ GCS (gs://funnelpulse-data-479512)            │
│   Compute: Local Spark │ GCP Dataproc                                       │
└─────────────────────────────────────────────────────────────────────────────┘
```

### Lakehouse Layers

| Layer      | Purpose                               | Storage | Partitioning            |
| ---------- | ------------------------------------- | ------- | ----------------------- |
| **Bronze** | Raw ingestion, minimal transformation | Parquet | `event_date`            |
| **Silver** | Cleaned, deduplicated, normalized     | Parquet | `event_date`            |
| **Gold**   | Business-level aggregations           | Parquet | `window_date` or `date` |

---

## 3. Project Structure

```
Big-Data-Project/
├── README.md                    # This documentation
├── config.py                    # Centralized configuration (local/GCP)
├── requirements.txt             # Python dependencies
├── setup.sh                     # Local environment setup
├── gcp_setup.sh                 # GCP one-click deployment ⭐
├── .gitignore                   # Git ignore rules
│
├── notebooks/                   # Jupyter notebooks (local development)
│   ├── 01_batch_bronze_silver_gold.ipynb
│   ├── 02_additional_gold_tables.ipynb
│   ├── 03_build_stream_input_from_bronze.ipynb
│   ├── 04_streaming_funnel_hourly_brand.ipynb
│   ├── 05_anomaly_detection_hourly_brand.ipynb
│   └── 06_visualizations.ipynb
│
├── gcp_jobs/                    # PySpark scripts for GCP Dataproc ⭐
│   ├── 01_batch_bronze_silver_gold.py
│   ├── 02_additional_gold_tables.py
│   ├── 03_build_stream_input.py
│   ├── 04_streaming_funnel.py
│   ├── 05_anomaly_detection.py
│   ├── 06_summary_report.py
│   ├── inspect_kafka_stream_gold.py
│   └── test_gcp_pipeline.py
│
├── kafka/                       # Kafka integration Spark scripts
│   ├── replay_from_bronze_spark.py
│   ├── funnel_funnel_hourly_brand_from_kafka.py
│   └── synthetic_producer.py
│
├── data_raw/                    # Raw CSV files (gitignored)
│   ├── 2019-Oct.csv
│   └── 2019-Nov.csv
│
├── tables/                      # Generated Parquet tables (gitignored)
│   ├── bronze_events/
│   ├── silver_events/
│   ├── gold_funnel_hourly_brand/
│   ├── gold_funnel_daily_brand/
│   ├── gold_funnel_daily_category/
│   ├── gold_funnel_hourly_price_band/
│   ├── gold_stream_funnel_hourly_brand/
│   └── gold_anomalies_hourly_brand/
│
├── stream_input/                # Streaming simulation files (gitignored)
├── checkpoints/                 # Spark streaming checkpoints (gitignored)
│
└── dashboard/                   # Streamlit dashboard application
    ├── app.py                   # Main overview page
    ├── pages/                   # Multi-page dashboard
    │   ├── 1_Brands.py         # Brand analytics
    │   ├── 2_Categories.py     # Category performance
    │   ├── 3_Price_Bands.py    # Price segment analysis
    │   └── 4_Anomalies.py      # Anomaly monitoring
    ├── utils/                   # Shared utilities
    ├── Dockerfile              # Cloud Run deployment
    └── deploy.sh               # Deployment script
```

---

## 4. Dataset

**Source**: [Kaggle - eCommerce Events History in Cosmetics Shop](https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop)

| Attribute        | Value                                    |
| ---------------- | ---------------------------------------- |
| **Total Events** | ~8.7 million                             |
| **Time Period**  | October - November 2019                  |
| **Event Types**  | view, cart, purchase, remove_from_cart   |
| **Dimensions**   | brand, category, price, user_id, session |
| **File Size**    | ~1.5 GB (CSV)                            |

### Sample Record

```json
{
	"event_time": "2019-10-01 00:00:00",
	"event_type": "view",
	"product_id": 5300797,
	"category_id": 2053013563173241677,
	"category_code": "electronics.smartphone",
	"brand": "samsung",
	"price": 274.85,
	"user_id": 541312140,
	"user_session": "72d76fde-8bb3-4e00-8c23-a032dfed738c"
}
```

---

## 5. Quick Start

### 5.1 Local Development

#### Prerequisites

- **Python 3.8+**
- **Java 17** (required for PySpark 3.4+)
- **~10GB disk space** for data and tables

#### Step-by-Step Setup

```bash
# 1. Clone the repository
git clone <repository-url>
cd Big-Data-Project

# 2. Run setup script (creates venv, installs dependencies)
./setup.sh

# 3. Activate virtual environment
source venv/bin/activate

# 4. Download dataset from Kaggle
# Option A: Using Kaggle CLI (requires ~/.kaggle/kaggle.json)
kaggle datasets download -d mkechinov/ecommerce-events-history-in-cosmetics-shop
unzip ecommerce-events-history-in-cosmetics-shop.zip -d data_raw/

# Option B: Manual download
# Visit: https://www.kaggle.com/datasets/mkechinov/ecommerce-events-history-in-cosmetics-shop
# Download and extract CSV files to data_raw/

# 5. Verify configuration
python config.py

# 6. Start Jupyter Lab
jupyter lab

# 7. Run notebooks in order (01 → 06)
```

#### Java 17 Installation

```bash
# macOS (Homebrew)
brew install openjdk@17
export JAVA_HOME=/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home

# Ubuntu/Debian
sudo apt install openjdk-17-jdk
export JAVA_HOME=/usr/lib/jvm/java-17-openjdk-amd64

# Verify
java -version  # Should show version 17.x
```

### 5.2 GCP Deployment

#### Prerequisites

- **GCP account** with billing enabled (~$50/month for light usage)
- **gcloud CLI** installed and authenticated
- **GCS bucket** created (or use existing `gs://funnelpulse-data-479512`)

#### One-Click Deployment ⭐

The `gcp_setup.sh` script automates the entire GCP deployment.

```bash
# Setup cluster + upload scripts (takes ~3 minutes)
./gcp_setup.sh

# Run the full BATCH + FILE-BASED streaming pipeline
./gcp_setup.sh --run-all

# Run the full KAFKA-BASED streaming pipeline
./gcp_setup.sh --run-kafka

# Check status of cluster, Kafka VM, and GCS bucket
./gcp_setup.sh --status

# Delete cluster and Kafka VM (preserves data in GCS)
./gcp_setup.sh --delete
```

- `--run-all`: Executes the original file-based batch and streaming pipeline.
- `--run-kafka`: Deploys a Kafka VM and runs the end-to-end Kafka-based streaming pipeline.

#### Manual GCP Setup

```bash
# 1. Authenticate
gcloud auth login

# 2. Set project
gcloud config set project funnelpulse-479512

# 3. Create bucket (if not exists)
gsutil mb -l us-central1 gs://funnelpulse-data-479512

# 4. Upload local tables to GCS (if running batch locally first)
gsutil -m cp -r tables/ gs://funnelpulse-data-479512/

# 5. Create Dataproc cluster
gcloud dataproc clusters create funnelpulse-cluster \
    --region=us-central1 \
    --zone=us-central1-a \
    --master-machine-type=n1-standard-2 \
    --master-boot-disk-size=50GB \
    --num-workers=2 \
    --worker-machine-type=n1-standard-2 \
    --worker-boot-disk-size=50GB \
    --image-version=2.1-debian11 \
    --optional-components=JUPYTER \
    --enable-component-gateway

# 6. Upload job scripts
gsutil cp gcp_jobs/*.py gs://funnelpulse-data-479512/jobs/

# 7. Submit a job
gcloud dataproc jobs submit pyspark \
    gs://funnelpulse-data-479512/jobs/06_summary_report.py \
    --cluster=funnelpulse-cluster --region=us-central1
```

---

## 6. Pipeline Components

### Notebook/Job Descriptions

| #   | Notebook / GCP Job                  | Purpose                                 | Input             | Output                                                   |
| --- | ----------------------------------- | --------------------------------------- | ----------------- | -------------------------------------------------------- |
| 01  | `batch_bronze_silver_gold`          | Core lakehouse pipeline                 | Raw CSV           | bronze, silver, gold_hourly_brand                        |
| 02  | `additional_gold_tables`            | Extended analytics views                | silver            | gold_daily_brand, gold_daily_category, gold_hourly_price |
| 03  | `build_stream_input`                | Prepare file-based streaming simulation | bronze            | stream_input/ (50 files)                                 |
| 04  | `streaming_funnel`                  | Real-time aggregation from files        | stream_input      | gold_stream_funnel_hourly_brand                          |
| 05  | `anomaly_detection`                 | Statistical anomaly flagging            | gold_hourly_brand | gold_anomalies_hourly_brand                              |
| 06  | `visualizations` / `summary_report` | Charts / stats                          | All gold tables   | Visual output                                            |

### Data Flow & Row Counts

```
Raw CSV (8.7M rows)
        │
        ▼
Bronze (8,738,120 rows)
        │
        ▼
Silver (8,260,755 rows) ──────┬──────────────────────┐
        │                     │                      │
        ▼                     ▼                      ▼
Gold Hourly Brand      Gold Daily Brand      Gold Daily Category
(183,145 rows)         (13,107 rows)         (374 rows)
        │                     │                      │
        │                     └──────────────────────┘
        ▼
Gold Hourly Price Band (5,856 rows)
        │
        ▼
Anomalies (442 rows)
```

---

## 7. Running the Pipeline

### 7.1 Local (Notebooks)

Run notebooks **in sequence** (dependencies exist):

```
01_batch_bronze_silver_gold.ipynb      ← Must run first
        ↓
02_additional_gold_tables.ipynb        ← Requires silver
        ↓
03_build_stream_input_from_bronze.ipynb ← Requires bronze
        ↓
04_streaming_funnel_hourly_brand.ipynb  ← Requires stream_input
        ↓
05_anomaly_detection_hourly_brand.ipynb ← Requires gold_hourly_brand
        ↓
06_visualizations.ipynb                 ← Requires all gold tables
```

### 7.2 GCP (Job Scripts)

Submit jobs using `gcloud dataproc jobs submit pyspark`:

```bash
# All jobs in sequence
gcloud dataproc jobs submit pyspark gs://funnelpulse-data-479512/jobs/01_batch_bronze_silver_gold.py \
    --cluster=funnelpulse-cluster --region=us-central1

gcloud dataproc jobs submit pyspark gs://funnelpulse-data-479512/jobs/02_additional_gold_tables.py \
    --cluster=funnelpulse-cluster --region=us-central1

gcloud dataproc jobs submit pyspark gs://funnelpulse-data-479512/jobs/03_build_stream_input.py \
    --cluster=funnelpulse-cluster --region=us-central1

gcloud dataproc jobs submit pyspark gs://funnelpulse-data-479512/jobs/04_streaming_funnel.py \
    --cluster=funnelpulse-cluster --region=us-central1

gcloud dataproc jobs submit pyspark gs://funnelpulse-data-479512/jobs/05_anomaly_detection.py \
    --cluster=funnelpulse-cluster --region=us-central1

gcloud dataproc jobs submit pyspark gs://funnelpulse-data-479512/jobs/06_summary_report.py \
    --cluster=funnelpulse-cluster --region=us-central1
```

Or use the one-click script: `./gcp_setup.sh --run-all`

### 7.3 GCP (Kafka Streaming Pipeline)

To run the end-to-end Kafka-based streaming pipeline, use the one-click script:

```bash
# Run the entire Kafka-based streaming pipeline
./gcp_setup.sh --run-kafka
```

This command orchestrates the entire process:

1.  **Replays** historical data from the `bronze` table into a Kafka topic (`funnelpulse_events`) using a Dataproc job.
2.  **Starts** a continuous structured streaming job on Dataproc to consume from Kafka and write to the `gold_stream_funnel_hourly_brand_kafka` GCS table.
3.  **Runs** an inspect job to validate the output table by printing its schema and row counts.

---

## 8. Configuration

### config.py

The `config.py` file provides centralized configuration for all environments:

```python
# Switch between environments
ENVIRONMENT = "local"  # Options: "local", "jupyterhub", "gcp"

# GCP Settings (used when ENVIRONMENT = "gcp")
GCS_BUCKET = "funnelpulse-data-479512"
GCS_PROJECT = "funnelpulse-479512"
GCS_REGION = "us-central1"

# Spark Settings
SPARK_CONFIG = {
    "app_name": "FunnelPulse",
    "master": "local[*]",      # Use all cores locally
    "driver_memory": "4g",
    "sql_shuffle_partitions": 200,
}

# Anomaly Detection Thresholds
ANOMALY_CONFIG = {
    "min_views_threshold": 10,  # Minimum views for baseline
    "z_score_threshold": 2.0,   # Z-score cutoff for anomalies
}
```

### Environment-Specific Paths

| Environment | Storage Path                                        |
| ----------- | --------------------------------------------------- |
| local       | `./tables/bronze_events`                            |
| gcp         | `gs://funnelpulse-data-479512/tables/bronze_events` |

---

## 9. Data Schema

### Bronze/Silver Events

| Column        | Type      | Description                            |
| ------------- | --------- | -------------------------------------- |
| event_time    | timestamp | Event timestamp                        |
| event_type    | string    | view, cart, purchase, remove_from_cart |
| product_id    | int       | Product identifier                     |
| category_id   | long      | Category identifier                    |
| category_code | string    | Category hierarchy                     |
| brand         | string    | Product brand                          |
| price         | double    | Product price                          |
| user_id       | int       | User identifier                        |
| user_session  | string    | Session UUID                           |
| event_date    | date      | Partition column                       |

### Silver Additional Columns

| Column             | Type    | Description                |
| ------------------ | ------- | -------------------------- |
| brand_norm         | string  | Lowercase normalized brand |
| category_code_norm | string  | Normalized category        |
| dq*missing*\*      | boolean | Data quality flags         |

### Gold Funnel Metrics

| Column                | Type      | Description            |
| --------------------- | --------- | ---------------------- |
| window_start          | timestamp | Hour window start      |
| window_end            | timestamp | Hour window end        |
| brand                 | string    | Brand name             |
| views                 | long      | View event count       |
| carts                 | long      | Cart event count       |
| purchases             | long      | Purchase event count   |
| revenue               | double    | Sum of purchase prices |
| view_to_cart_rate     | double    | carts / views          |
| cart_to_purchase_rate | double    | purchases / carts      |
| conversion_rate       | double    | purchases / views      |

---

## 10. Anomaly Detection

### Algorithm

FunnelPulse uses **Z-score based anomaly detection**:

1. **Compute Baselines**:

   - Per-brand: mean and stddev of conversion rate across all hours
   - Per-brand-hour: mean and stddev at each hour of day (0-23)

2. **Calculate Z-scores**:

   ```
   z_brand = (current_conversion - brand_mean) / brand_std
   z_brand_hour = (current_conversion - brand_hour_mean) / brand_hour_std
   ```

3. **Flag Anomalies**:
   - **Drop**: z_score ≤ -2.0 (conversion significantly lower than normal)
   - **Spike**: z_score ≥ +2.0 (conversion significantly higher than normal)
   - Requires minimum 50 views to flag

### Sample Output

```
+-------------------+--------+-----+----------+-----------------+--------+
|window_start       |brand   |views|conversion|conv_mean_brand  |z_brand |
+-------------------+--------+-----+----------+-----------------+--------+
|2019-10-15 12:00:00|runail  |156  |0.0128    |0.200            |-2.34   | ← DROP
|2019-10-20 18:00:00|grattol |89   |0.505     |0.119            |+3.12   | ← SPIKE
+-------------------+--------+-----+----------+-----------------+--------+
```

---

## 11. GCP Cost & Resource Management

### Cost Estimate

| Resource             | Configuration | Hourly Cost   | Monthly (8hr/day) |
| -------------------- | ------------- | ------------- | ----------------- |
| Dataproc Master      | n1-standard-2 | ~$0.10        | ~$17              |
| Dataproc Workers (2) | n1-standard-2 | ~$0.20        | ~$34              |
| Kafka VM (GCE)       | e2-standard-2 | ~$0.07        | ~$16              |
| GCS Storage          | ~1GB          | ~$0.02        | ~$0.02            |
| **Total**            |               | **~$0.39/hr** | **~$67/month**    |

Note: Running the Kafka pipeline (`--run-kafka`) provisions an additional GCE virtual machine (`kafka-broker-1`) which incurs costs.

### Cost-Saving Tips & Cleanup

#### Cleanup Checklist

The primary method for cleaning up resources is the `gcp_setup.sh` script.

```bash
# Always delete cluster AND Kafka VM when not in use!
./gcp_setup.sh --delete
```

This command is designed to remove all major cost-incurring resources:

- **Deletes the Dataproc cluster**: This is a primary driver of cost.
- **Deletes the Kafka VM**: The `kafka-broker-1` GCE instance is also removed.
- **Preserves GCS data**: Your data in `gs://funnelpulse-data-479512/` remains untouched.

#### Verification Commands

After running the delete script, you can manually verify that all billable compute resources have been removed using the following commands. If the output for each is empty, you are safe from ongoing compute costs.

```bash
# 1. Verify no Dataproc clusters are running
gcloud dataproc clusters list --region=us-central1
# Expected: Listed 0 items.

# 2. Verify no Compute Engine VMs are running
gcloud compute instances list
# Expected: Listed 0 items.

# 3. Verify no orphaned persistent disks remain
gcloud compute disks list
# Expected: Listed 0 items.

# 4. Verify no reserved external IP addresses remain
gcloud compute addresses list
# Expected: Listed 0 items.
```

---

## 12. Troubleshooting

### Common Issues

| Issue                | Symptom                                                 | Solution                                    |
| -------------------- | ------------------------------------------------------- | ------------------------------------------- |
| **Java Version**     | `UnsupportedClassVersionError: class file version 61.0` | Install Java 17 and set JAVA_HOME           |
| **Path with Spaces** | `No such file or directory`                             | Use Python `os` module instead of bash      |
| **GCS Permission**   | `AccessDeniedException: 403`                            | Grant Storage Admin role to service account |
| **Port Conflict**    | `Service 'SparkUI' could not bind on port 4040`         | Normal - Spark auto-finds next port         |
| **Out of Memory**    | `java.lang.OutOfMemoryError`                            | Increase `driver_memory` in config.py       |

### I want to make sure I am not being charged

To ensure you are not incurring unexpected GCP costs, follow these steps:

1.  **Run the cleanup script**: This is the most important step.
    ```bash
    ./gcp_setup.sh --delete
    ```
2.  **Verify resource deletion**: Use the verification commands listed in **Section 11 (Cleanup Checklist)**. If the output of the `gcloud` list commands for clusters, instances, disks, and addresses is empty, you have successfully removed all major compute resources.

### Debugging Commands

```bash
# Check GCS bucket contents
gsutil ls -la gs://funnelpulse-data-479512/tables/

# View Dataproc job logs
gcloud dataproc jobs list --region=us-central1 --state-filter=ACTIVE
gcloud dataproc jobs describe <JOB_ID> --region=us-central1

# Check cluster status
gcloud dataproc clusters describe funnelpulse-cluster --region=us-central1
```

---

## 13. Future Enhancements

### Recommended Next Steps

| Priority   | Enhancement              | Description                                                              |
| ---------- | ------------------------ | ------------------------------------------------------------------------ |
| **HIGH**   | Harden Kafka Integration | Productionize Kafka (security, HA, managed service like Confluent Cloud) |
| **HIGH**   | Alerting System          | Email/Slack notifications for anomalies                                  |
| **MEDIUM** | Dashboard                | Streamlit interactive visualization - IMPLEMENTED                        |
| **MEDIUM** | ML Anomaly Detection     | Isolation Forest, LSTM for better detection                              |
| **LOW**    | CI/CD                    | GitHub Actions, Terraform for infrastructure                             |
| **LOW**    | Data Quality             | Great Expectations integration                                           |

### Production Architecture Vision

```
┌─────────────────────────────────────────────────────────────┐
│                   Production Architecture                    │
├─────────────────────────────────────────────────────────────┤
│                                                             │
│  [Web/Mobile Apps] ──▶ [Kafka] ──▶ [Spark Streaming]       │
│                           │              │                  │
│                           ▼              ▼                  │
│                      [Raw Topic]   [Gold Tables]           │
│                           │              │                  │
│                           ▼              ▼                  │
│                    [Data Lake]    [Dashboard/Alerts]       │
│                    (Bronze/Silver)                          │
│                                                             │
└─────────────────────────────────────────────────────────────┘
```

---

## 14. Kafka Integration (For Production)

The project now includes a fully functional Kafka-based streaming pipeline on GCP, orchestrated by the `gcp_setup.sh --run-kafka` command. This provides a production-grade alternative to the file-based streaming simulation.

### Architecture

The Kafka integration consists of:

- A **GCE Virtual Machine** (`kafka-broker-1`) running Zookeeper and Kafka in Docker containers.
- A Spark job (`replay_from_bronze_spark.py`) to replay historical bronze data into the `funnelpulse_events` Kafka topic.
- A continuous Spark Structured Streaming job (`funnel_funnel_hourly_brand_from_kafka.py`) that consumes from Kafka.
- A new gold table in GCS: `gold_stream_funnel_hourly_brand_kafka`.

This setup allows Dataproc and the Kafka VM to communicate efficiently over GCP's internal network.

### Scripts

The core scripts for this pipeline are located in the `kafka/` and `gcp_jobs/` directories:

| Script                                     | Location    | Purpose                                                                                                                                  |
| ------------------------------------------ | ----------- | ---------------------------------------------------------------------------------------------------------------------------------------- |
| `replay_from_bronze_spark.py`              | `kafka/`    | A Spark batch job that reads from the bronze table and publishes events to a Kafka topic. Replaces `03_build_stream_input.py`.           |
| `funnel_funnel_hourly_brand_from_kafka.py` | `kafka/`    | A Spark streaming job that reads from Kafka, computes funnel metrics, and writes to a gold GCS table. Replaces `04_streaming_funnel.py`. |
| `inspect_kafka_stream_gold.py`             | `gcp_jobs/` | A utility job to read and display the contents of the Kafka-based streaming gold table for validation.                                   |

### Configuration Snippets

Key configuration is handled automatically by `gcp_setup.sh`, which passes the internal Kafka address (`kafka-broker-1:9092`) to the Dataproc jobs.

```python
# From funnel_funnel_hourly_brand_from_kafka.py
# Note: KAFKA_BOOTSTRAP_SERVERS is injected via environment variable on GCP
KAFKA_BOOTSTRAP_SERVERS = os.getenv("KAFKA_BOOTSTRAP_SERVERS", "localhost:9092")
KAFKA_TOPIC = "funnelpulse_events"

df_kafka = (
    spark.readStream.format("kafka")
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS)
    .option("subscribe", KAFKA_TOPIC)
    .load()
)
```

### Migration Path

The Kafka pipeline is now the primary streaming implementation on GCP. The file-based approach remains for local development and basic validation.

| Environment       | Old (File-based)              | New (Kafka-based on GCP)                         |
| ----------------- | ----------------------------- | ------------------------------------------------ |
| **Data Source**   | `stream_input/` Parquet files | `funnelpulse_events` Kafka Topic                 |
| **Replay Job**    | `03_build_stream_input.py`    | `kafka/replay_from_bronze_spark.py`              |
| **Streaming Job** | `04_streaming_funnel.py`      | `kafka/funnel_funnel_hourly_brand_from_kafka.py` |

The cleaning logic, aggregations, and anomaly detection remain unchanged - only the transport layer is different.

---

## 15. Dashboard

The FunnelPulse Dashboard is a multi-page Streamlit application for interactive visualization of funnel metrics.

### Features

| Page | Description |
|------|-------------|
| **Overview** | KPI cards, conversion funnel, overall metrics |
| **Brands** | Brand performance, revenue trends, conversion charts |
| **Categories** | Category-level analysis and comparisons |
| **Price Bands** | Performance by price segment |
| **Anomalies** | Anomaly timeline, alerts table, brand deep-dive |

### Local Development

```bash
cd dashboard

# Install dependencies (using uv)
uv sync

# Run the dashboard
uv run streamlit run app.py

# Or with standard Python
pip install -r requirements.txt
streamlit run app.py
```

The dashboard will be available at `http://localhost:8501`.

### Cloud Run Deployment

Deploy to Google Cloud Run for production use:

```bash
cd dashboard

# Set your GCP project (if different from default)
export GCP_PROJECT="your-project-id"
export GCS_BUCKET="your-bucket-name"

# Deploy
./deploy.sh
```

The deployment:
- Uses Cloud Run's free tier (2M requests/month)
- Reads data from GCS bucket
- Auto-scales based on traffic

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `ENVIRONMENT` | `local` | `local` or `gcp` |
| `GCS_BUCKET` | `funnelpulse-ss18851-data` | GCS bucket for data |

---

## Summary

FunnelPulse provides a complete, production-ready foundation for e-commerce funnel analytics:

| Component                               | Status                                   |
| --------------------------------------- | ---------------------------------------- |
| Batch Pipeline (Bronze → Silver → Gold) | ✅ Complete                              |
| Streaming Pipeline                      | ✅ Complete (file-based)                 |
| Anomaly Detection                       | ✅ Complete                              |
| Local Development                       | ✅ Working                               |
| GCP Dataproc Deployment                 | ✅ Working                               |
| One-Click Setup Script                  | ✅ Available                             |
| Kafka Integration                       | ✅ Implemented (GCP Dataproc + Kafka VM) |
| Interactive Dashboard                   | ✅ Implemented (Streamlit + Cloud Run)   |

**Next Developer Focus Areas**:

1.  Add alerting/notification system
2.  Enhance anomaly detection with ML models

---

## License

MIT License - See [LICENSE](LICENSE) for details.

---

## Acknowledgments

- Dataset: [REES46 Marketing Platform](https://rees46.com/) via Kaggle
- Apache Spark and PySpark community
- Google Cloud Platform

---

_Built with Apache Spark on Google Cloud Platform_
