{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Anomaly Detection - Demonstration\n",
    "\n",
    "This notebook demonstrates the advanced anomaly detection system with multiple models:\n",
    "- LSTM Autoencoder for temporal patterns\n",
    "- Prophet for seasonality detection\n",
    "- Isolation Forest for multivariate outliers\n",
    "- Changepoint detection for structural breaks\n",
    "- Ensemble framework combining all models\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')\n",
    "%matplotlib inline\n",
    "\n",
    "# Add advanced_anomaly_detection to path\n",
    "sys.path.insert(0, os.path.join(os.getcwd(), '..', 'advanced_anomaly_detection'))\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Advanced Anomaly Detection Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Load the gold funnel hourly brand data from the existing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "TABLES_DIR = \"../tables\"  # Local path\n",
    "# For GCS: TABLES_DIR = \"gs://funnelpulse-ss18851-data/tables\"\n",
    "\n",
    "GOLD_HOURLY_BRAND_PATH = f\"{TABLES_DIR}/gold_funnel_hourly_brand\"\n",
    "\n",
    "# Load data\n",
    "df = spark.read.parquet(GOLD_HOURLY_BRAND_PATH)\n",
    "\n",
    "print(f\"Total records: {df.count():,}\")\n",
    "print(f\"\\nSchema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nSample data:\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for reliable data\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "MIN_VIEWS = 20\n",
    "\n",
    "df_filtered = df.filter(\n",
    "    (col(\"views\") >= MIN_VIEWS) & \n",
    "    (col(\"brand\").isNotNull())\n",
    ")\n",
    "\n",
    "print(f\"Filtered records: {df_filtered.count():,}\")\n",
    "print(f\"Number of brands: {df_filtered.select('brand').distinct().count()}\")\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\nConversion rate statistics:\")\n",
    "df_filtered.select('conversion_rate').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Advanced Pipeline\n",
    "\n",
    "Create the advanced anomaly detection pipeline with all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from advanced_anomaly_pipeline import create_pipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = create_pipeline(spark)\n",
    "\n",
    "print(\"\\nPipeline initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Apply advanced feature engineering to create multi-dimensional features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering\n",
    "df_features = pipeline.prepare_features(df_filtered)\n",
    "\n",
    "print(f\"\\nFeatures added. New column count: {len(df_features.columns)}\")\n",
    "print(\"\\nNew columns:\")\n",
    "for col_name in df_features.columns:\n",
    "    if col_name not in df.columns:\n",
    "        print(f\"  - {col_name}\")\n",
    "\n",
    "# Show sample with new features\n",
    "print(\"\\nSample with features:\")\n",
    "df_features.select(\n",
    "    'window_start', 'brand', 'conversion_rate',\n",
    "    'hour_of_day', 'day_of_week', 'is_weekend',\n",
    "    'rolling_7d_conversion', 'conversion_z_score'\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train Models\n",
    "\n",
    "Train all available anomaly detection models on historical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models\n",
    "training_stats = pipeline.train_models(df_features, train_split=0.8)\n",
    "\n",
    "# Display training statistics\n",
    "print(\"\\nTraining Statistics:\")\n",
    "print(\"=\" * 60)\n",
    "for model_name, stats in training_stats.items():\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    if isinstance(stats, dict):\n",
    "        for key, value in stats.items():\n",
    "            if key != 'history':\n",
    "                print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Detect Anomalies\n",
    "\n",
    "Run all models to detect anomalies in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect anomalies\n",
    "df_anomalies = pipeline.detect_anomalies(df_features)\n",
    "\n",
    "print(\"Anomaly detection complete!\")\n",
    "print(f\"\\nTotal records analyzed: {len(df_anomalies):,}\")\n",
    "\n",
    "# Check which models detected anomalies\n",
    "if 'lstm_is_anomaly' in df_anomalies.columns:\n",
    "    print(f\"LSTM anomalies: {df_anomalies['lstm_is_anomaly'].sum()}\")\n",
    "if 'prophet_is_anomaly' in df_anomalies.columns:\n",
    "    print(f\"Prophet anomalies: {df_anomalies['prophet_is_anomaly'].sum()}\")\n",
    "if 'isolation_forest_is_anomaly' in df_anomalies.columns:\n",
    "    print(f\"Isolation Forest anomalies: {df_anomalies['isolation_forest_is_anomaly'].sum()}\")\n",
    "if 'changepoint_is_anomaly' in df_anomalies.columns:\n",
    "    print(f\"Changepoint anomalies: {df_anomalies['changepoint_is_anomaly'].sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Apply Ensemble\n",
    "\n",
    "Combine predictions from all models using weighted voting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply ensemble\n",
    "df_ensemble = pipeline.apply_ensemble(df_anomalies)\n",
    "\n",
    "# Get summary statistics\n",
    "summary = pipeline.ensemble.get_summary_statistics(df_ensemble)\n",
    "\n",
    "print(\"\\nEnsemble Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total records: {summary['total_records']:,}\")\n",
    "print(f\"Total anomalies: {summary['total_anomalies']:,}\")\n",
    "print(f\"Anomaly rate: {summary['anomaly_rate']:.2%}\")\n",
    "print(f\"\\nBy severity: {summary['by_severity']}\")\n",
    "print(f\"By type: {summary['by_type']}\")\n",
    "\n",
    "if 'total_estimated_revenue_impact' in summary:\n",
    "    print(f\"\\nEstimated revenue impact: ${summary['total_estimated_revenue_impact']:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Results\n",
    "\n",
    "Create visualizations to understand anomaly patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to anomalies only\n",
    "anomalies = df_ensemble[df_ensemble['is_anomaly']].copy()\n",
    "\n",
    "if len(anomalies) > 0:\n",
    "    # Plot 1: Anomaly severity distribution\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Severity distribution\n",
    "    severity_counts = anomalies['anomaly_severity'].value_counts()\n",
    "    axes[0, 0].bar(severity_counts.index, severity_counts.values, color='coral')\n",
    "    axes[0, 0].set_title('Anomalies by Severity')\n",
    "    axes[0, 0].set_xlabel('Severity')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    \n",
    "    # Type distribution\n",
    "    type_counts = anomalies['anomaly_type'].value_counts()\n",
    "    axes[0, 1].bar(type_counts.index, type_counts.values, color='skyblue')\n",
    "    axes[0, 1].set_title('Anomalies by Type')\n",
    "    axes[0, 1].set_xlabel('Type')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    \n",
    "    # Anomaly score distribution\n",
    "    axes[1, 0].hist(anomalies['ensemble_anomaly_score'], bins=20, color='lightgreen', edgecolor='black')\n",
    "    axes[1, 0].set_title('Ensemble Anomaly Score Distribution')\n",
    "    axes[1, 0].set_xlabel('Anomaly Score')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    \n",
    "    # Top affected brands\n",
    "    top_brands = anomalies['brand'].value_counts().head(10)\n",
    "    axes[1, 1].barh(top_brands.index, top_brands.values, color='mediumpurple')\n",
    "    axes[1, 1].set_title('Top 10 Brands by Anomaly Count')\n",
    "    axes[1, 1].set_xlabel('Anomaly Count')\n",
    "    axes[1, 1].set_ylabel('Brand')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No anomalies to visualize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot 2: Time series with anomalies for a sample brand\n",
    "if len(anomalies) > 0:\n",
    "    sample_brand = anomalies['brand'].value_counts().index[0]\n",
    "    brand_data = df_ensemble[df_ensemble['brand'] == sample_brand].sort_values('window_start')\n",
    "    brand_anomalies = brand_data[brand_data['is_anomaly']]\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot conversion rate\n",
    "    plt.plot(brand_data['window_start'], brand_data['conversion_rate'], \n",
    "             label='Actual', color='blue', alpha=0.7, linewidth=1)\n",
    "    \n",
    "    # Plot expected value if available\n",
    "    if 'expected_value' in brand_data.columns:\n",
    "        plt.plot(brand_data['window_start'], brand_data['expected_value'], \n",
    "                 label='Expected', color='green', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Highlight anomalies\n",
    "    plt.scatter(brand_anomalies['window_start'], brand_anomalies['conversion_rate'],\n",
    "                color='red', s=100, zorder=5, label='Anomalies', marker='X')\n",
    "    \n",
    "    plt.title(f'Conversion Rate Over Time - {sample_brand}')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Conversion Rate')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nShowing {len(brand_anomalies)} anomalies for brand: {sample_brand}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison\n",
    "\n",
    "Compare the performance of individual models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model score comparison\n",
    "model_columns = [\n",
    "    'lstm_anomaly_score',\n",
    "    'prophet_anomaly_score', \n",
    "    'isolation_forest_anomaly_score',\n",
    "    'changepoint_anomaly_score'\n",
    "]\n",
    "\n",
    "available_models = [col for col in model_columns if col in df_ensemble.columns]\n",
    "\n",
    "if available_models:\n",
    "    # Box plot of model scores\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    data_to_plot = [df_ensemble[col].dropna() for col in available_models]\n",
    "    labels = [col.replace('_anomaly_score', '').replace('_', ' ').title() for col in available_models]\n",
    "    \n",
    "    ax.boxplot(data_to_plot, labels=labels)\n",
    "    ax.set_title('Model Score Distribution Comparison')\n",
    "    ax.set_ylabel('Anomaly Score')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Correlation between models\n",
    "    if len(available_models) > 1:\n",
    "        correlation = df_ensemble[available_models].corr()\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(correlation, annot=True, cmap='coolwarm', center=0,\n",
    "                    xticklabels=labels, yticklabels=labels)\n",
    "        plt.title('Model Score Correlation')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Root Cause Analysis\n",
    "\n",
    "Analyze root causes for critical anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get critical anomalies\n",
    "critical_anomalies = df_ensemble[\n",
    "    (df_ensemble['is_anomaly']) & \n",
    "    (df_ensemble['anomaly_severity'] == 'critical')\n",
    "].copy()\n",
    "\n",
    "if len(critical_anomalies) > 0:\n",
    "    print(f\"Found {len(critical_anomalies)} critical anomalies\\n\")\n",
    "    \n",
    "    # Perform RCA on top 5\n",
    "    for idx, row in critical_anomalies.head(5).iterrows():\n",
    "        rca = pipeline.analytics.perform_root_cause_analysis(row, df_ensemble)\n",
    "        \n",
    "        print(\"=\"*60)\n",
    "        print(f\"Brand: {rca['brand']}\")\n",
    "        print(f\"Time: {rca['timestamp']}\")\n",
    "        print(f\"Type: {rca['anomaly_type']} | Severity: {rca['severity']}\")\n",
    "        print(f\"\\nSummary: {rca['summary']}\")\n",
    "        print(f\"\\nTop Contributing Factors:\")\n",
    "        for factor in rca['primary_factors']:\n",
    "            print(f\"  - {factor['metric']}: {factor['change_percent']:.1f}%\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No critical anomalies found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results\n",
    "\n",
    "Export anomaly detection results for further analysis or reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export anomalies to CSV\n",
    "output_path = \"../output/anomalies_advanced.csv\"\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "# Select key columns for export\n",
    "export_columns = [\n",
    "    'window_start', 'brand', 'anomaly_type', 'anomaly_severity',\n",
    "    'conversion_rate', 'expected_value', 'deviation_percentage',\n",
    "    'views', 'purchases', 'revenue',\n",
    "    'ensemble_anomaly_score', 'anomaly_confidence',\n",
    "    'estimated_revenue_impact', 'estimated_conversion_loss'\n",
    "]\n",
    "\n",
    "available_export_cols = [col for col in export_columns if col in anomalies.columns]\n",
    "export_df = anomalies[available_export_cols]\n",
    "\n",
    "export_df.to_csv(output_path, index=False)\n",
    "print(f\"Exported {len(export_df)} anomalies to {output_path}\")\n",
    "\n",
    "# Display summary\n",
    "print(\"\\nExported Data Summary:\")\n",
    "print(export_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Total Anomalies Detected**: Compare with baseline Z-score method\n",
    "2. **Model Performance**: Individual model contributions to ensemble\n",
    "3. **Business Impact**: Estimated revenue and conversion losses\n",
    "4. **Top Issues**: Brands and time periods with most anomalies\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Review critical anomalies with business stakeholders\n",
    "2. Set up automated alerting for high-severity anomalies\n",
    "3. Tune model parameters based on false positive feedback\n",
    "4. Deploy to production with real-time streaming\n",
    "5. Monitor model performance over time\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Advanced Anomaly Detection README](../advanced_anomaly_detection/README_advanced_anomaly.md)\n",
    "- [Configuration Guide](../advanced_anomaly_detection/config/anomaly_config.py)\n",
    "- [Model Documentation](../advanced_anomaly_detection/advanced_models/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Notebook complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
