{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f641010c-9dd4-4a3c-8c1e-3aa75eabf214",
   "metadata": {},
   "source": [
    "# Notebook 03 · Build Streaming Input from Bronze (Log Replay Setup)\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook prepares the **streaming input** for FunnelPulse by transforming historical events stored in the **bronze** layer into a file-based “event stream”.\n",
    "\n",
    "Because we are working in a classroom environment without a production Kafka cluster, we simulate a real-time event firehose by:\n",
    "\n",
    "- Selecting a time range of historical events from `bronze_events`\n",
    "- Ordering and repartitioning those events into many small Parquet files\n",
    "- Writing them into a `stream_input/` directory that Spark Structured Streaming can treat as a source\n",
    "\n",
    "This notebook does **not** perform any analytics itself. Its only job is to produce a realistic, incremental input for the streaming pipeline implemented in Notebook 04.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs and Outputs\n",
    "\n",
    "**Input table**\n",
    "\n",
    "- `tables/bronze_events`\n",
    "  - Built in Notebook 01\n",
    "  - Contains raw but normalized events for October and November\n",
    "  - Partitioned by `event_date`\n",
    "\n",
    "**Output directory**\n",
    "\n",
    "- `stream_input/`\n",
    "  - Located under the project root, e.g. `~/funnelpulse/stream_input/`\n",
    "  - Contains many small Parquet files\n",
    "  - Each file holds a subset of bronze events in the chosen date range\n",
    "  - Used as a streaming source in Notebook 04\n",
    "\n",
    "---\n",
    "\n",
    "## High Level Workflow\n",
    "\n",
    "1. Initialize Spark and project paths\n",
    "2. Load the bronze event table covering October and November\n",
    "3. Select a **streaming window** (a subset of days) to simulate as “live traffic”\n",
    "4. Repartition this subset into many small files\n",
    "5. Write the subset as Parquet files into `stream_input/`\n",
    "6. Optionally inspect the number of files and sample rows for sanity\n",
    "\n",
    "---\n",
    "\n",
    "## Streaming Simulation Design\n",
    "\n",
    "In a production deployment, FunnelPulse would consume events from a real-time source such as Kafka or a log service. On this environment, we approximate that behavior with a **file-based streaming source**:\n",
    "\n",
    "1. **Choose a historical period**\n",
    "   - For example, focus on events between:\n",
    "     - `2019-10-15` and `2019-10-31`\n",
    "   - This gives enough days and volume to exercise the streaming pipeline without overwhelming the cluster\n",
    "\n",
    "2. **Subset bronze events**\n",
    "   - Filter `bronze_events` to keep only events whose `event_date` falls within the chosen range\n",
    "   - Preserve all other columns so the streaming job sees the same schema as the batch pipeline\n",
    "\n",
    "3. **Chunk the data into many small files**\n",
    "   - Repartition the subset into a target number of partitions (e.g., 50)\n",
    "   - Each partition becomes a separate Parquet file in `stream_input/`\n",
    "   - This allows Spark Structured Streaming to ingest new files incrementally, one or a few at a time\n",
    "\n",
    "4. **Write to the streaming input directory**\n",
    "   - Use `mode(\"overwrite\")` so the streaming input can be rebuilt cleanly\n",
    "   - The resulting directory contains a backlog of “to-be-streamed” events\n",
    "\n",
    "---\n",
    "\n",
    "## How This Is Used by the Streaming Pipeline\n",
    "\n",
    "The streaming notebook (Notebook 04) uses `stream_input/` as a **file-based streaming source**:\n",
    "\n",
    "- It defines a streaming DataFrame with:\n",
    "  - Schema taken from `bronze_events`\n",
    "  - A `maxFilesPerTrigger` setting so that Spark processes one new file per microbatch\n",
    "- As the streaming query runs, Spark:\n",
    "  - Monitors `stream_input/` for new files\n",
    "  - Treats each file’s contents as the next batch of events\n",
    "  - Applies the same cleaning and aggregation logic used in the batch pipeline, but in streaming mode\n",
    "\n",
    "This log replay pattern closely mirrors how a Kafka-based or log-based streaming system behaves, but remains achievable within the constraints of the course environment.\n",
    "\n",
    "---\n",
    "\n",
    "## Role of This Notebook in the Overall System\n",
    "\n",
    "Within the FunnelPulse architecture, this notebook serves as the **bridge between batch history and streaming simulation**:\n",
    "\n",
    "- Notebooks 01 and 02:\n",
    "  - Build the **batch lakehouse**, including bronze, silver, and multiple gold tables\n",
    "- Notebook 03:\n",
    "  - Converts a slice of the bronze history into a pseudo-real-time **stream input**\n",
    "  - Makes it possible to test and demonstrate the streaming pipeline without external infrastructure\n",
    "- Notebook 04:\n",
    "  - Consumes `stream_input/` with Spark Structured Streaming\n",
    "  - Computes real-time hourly funnel metrics by brand, analogous to the batch gold table\n",
    "- Later notebooks:\n",
    "  - Use the gold metrics for anomaly detection and incident surfacing\n",
    "\n",
    "In short, Notebook 03 is an operational preparation step that turns static historical data into a sequence of events that the streaming system can consume as if they were arriving live."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea39376b-db06-41e9-9fd5-056d16aeb1d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x11267e3c0>\n",
      "Spark UI available at: http://localhost:4040\n",
      "Bronze path      : /Users/aranyaaryaman/Desktop/bigData 2/finalProject/Big-Data-Project/tables/bronze_events\n",
      "Stream input path: /Users/aranyaaryaman/Desktop/bigData 2/finalProject/Big-Data-Project/stream_input\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Spark initialization and paths for streaming input builder\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set JAVA_HOME to Java 17 (required for PySpark 3.4+)\n",
    "os.environ[\"JAVA_HOME\"] = \"/opt/homebrew/opt/openjdk@17/libexec/openjdk.jdk/Contents/Home\"\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, os.path.dirname(os.getcwd()))\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session for local execution\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"FunnelPulse Build Stream Input\")\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "print(spark)\n",
    "print(f\"Spark UI available at: http://localhost:4040\")\n",
    "\n",
    "# Project paths (parent of notebooks folder)\n",
    "project_root = os.path.dirname(os.getcwd())\n",
    "tables_dir = os.path.join(project_root, \"tables\")\n",
    "\n",
    "bronze_path       = os.path.join(tables_dir, \"bronze_events\")\n",
    "stream_input_path = os.path.join(project_root, \"stream_input\")\n",
    "\n",
    "# Ensure stream_input directory exists\n",
    "os.makedirs(stream_input_path, exist_ok=True)\n",
    "\n",
    "print(\"Bronze path      :\", bronze_path)\n",
    "print(\"Stream input path:\", stream_input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ae0755f-c92f-4816-8028-516be0ea1cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total BRONZE rows (Oct+Nov): 8738120\n",
      "+-------------------+----------+----------+\n",
      "|event_time         |event_date|event_type|\n",
      "+-------------------+----------+----------+\n",
      "|2019-11-22 00:00:00|2019-11-22|cart      |\n",
      "|2019-11-22 00:00:00|2019-11-22|view      |\n",
      "|2019-11-22 00:00:00|2019-11-22|cart      |\n",
      "|2019-11-22 00:00:01|2019-11-22|view      |\n",
      "|2019-11-22 00:00:01|2019-11-22|cart      |\n",
      "+-------------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "Rows in streaming subset: 2140953\n",
      "+-------------------+----------------+----------+-------------------+-------------+--------+-----+---------+------------------------------------+----------+\n",
      "|event_time         |event_type      |product_id|category_id        |category_code|brand   |price|user_id  |user_session                        |event_date|\n",
      "+-------------------+----------------+----------+-------------------+-------------+--------+-----+---------+------------------------------------+----------+\n",
      "|2019-10-15 00:00:01|view            |5736501   |1487580005050352469|NULL         |haruyama|3.97 |333916427|001b631b-ce89-4c73-85dc-f4377bdc69a8|2019-10-15|\n",
      "|2019-10-15 00:00:03|cart            |5767921   |1487580005595612013|NULL         |NULL    |2.05 |551819168|164f737d-6c0a-4816-bc68-5010ca29a9fd|2019-10-15|\n",
      "|2019-10-15 00:00:04|remove_from_cart|5684927   |1487580013841613016|NULL         |estel   |5.79 |546365792|31abc766-0225-4561-9cea-c5ddb96aaa8c|2019-10-15|\n",
      "|2019-10-15 00:00:05|view            |5664641   |1487580011425693811|NULL         |NULL    |0.7  |528559854|3ce26a14-2837-42e7-9ac1-bc575b68a958|2019-10-15|\n",
      "|2019-10-15 00:00:05|remove_from_cart|5561462   |1487580007952810971|NULL         |NULL    |8.65 |534191792|d90d7339-7d4a-4777-b38b-a07904877a83|2019-10-15|\n",
      "|2019-10-15 00:00:05|remove_from_cart|5561462   |1487580007952810971|NULL         |NULL    |8.65 |534191792|d90d7339-7d4a-4777-b38b-a07904877a83|2019-10-15|\n",
      "|2019-10-15 00:00:06|view            |5585656   |1487580007256556476|NULL         |NULL    |0.52 |473211243|633a8f02-0190-4483-9cf8-651e8d0a37e1|2019-10-15|\n",
      "|2019-10-15 00:00:06|view            |5885421   |1487580005092295511|NULL         |grattol |6.27 |541061775|1d1dcecc-572d-48f9-94b8-c63398c8bee3|2019-10-15|\n",
      "|2019-10-15 00:00:06|remove_from_cart|5684928   |1487580013841613016|NULL         |estel   |5.79 |546365792|31abc766-0225-4561-9cea-c5ddb96aaa8c|2019-10-15|\n",
      "|2019-10-15 00:00:09|view            |5685798   |1487580009445982239|NULL         |irisk   |0.16 |558390967|74455bdb-1bf2-4e04-be33-94d7f69124c8|2019-10-15|\n",
      "+-------------------+----------------+----------+-------------------+-------------+--------+-----+---------+------------------------------------+----------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Load bronze and filter a subset period for streaming\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "bronze = spark.read.parquet(bronze_path)\n",
    "\n",
    "print(\"Total BRONZE rows (Oct+Nov):\", bronze.count())\n",
    "bronze.select(\"event_time\", \"event_date\", \"event_type\").show(5, truncate=False)\n",
    "\n",
    "# For streaming demo, let's take events between 2019-10-15 and 2019-10-31 (example)\n",
    "bronze_stream_subset = bronze.filter(\n",
    "    (col(\"event_date\") >= \"2019-10-15\") & (col(\"event_date\") <= \"2019-10-31\")\n",
    ")\n",
    "\n",
    "print(\"Rows in streaming subset:\", bronze_stream_subset.count())\n",
    "bronze_stream_subset.orderBy(\"event_time\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11ee9bd3-b465-4c64-83f3-f49d97d649b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:======================================================> (49 + 1) / 50]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote streaming subset to: /Users/aranyaaryaman/Desktop/bigData 2/finalProject/Big-Data-Project/stream_input\n",
      "Number of parquet files in stream_input: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# CELL 3: Write streaming subset as many small Parquet files\n",
    "\n",
    "# Repartition to produce multiple small files (e.g., 50)\n",
    "# Adjust 50 up/down depending on size; more partitions = more \"micro-batches\"\n",
    "num_partitions = 50\n",
    "\n",
    "(\n",
    "    bronze_stream_subset\n",
    "    .repartition(num_partitions)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .parquet(stream_input_path)\n",
    ")\n",
    "\n",
    "print(\"Wrote streaming subset to:\", stream_input_path)\n",
    "\n",
    "# Quick inspection: count files (handles paths with spaces)\n",
    "import os\n",
    "files = [f for f in os.listdir(stream_input_path) if f.endswith('.parquet')]\n",
    "print(\"Number of parquet files in stream_input:\", len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c6ab38-d470-4516-b94f-bea82958c62f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
