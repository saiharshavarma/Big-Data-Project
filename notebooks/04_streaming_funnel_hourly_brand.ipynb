{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "045c3baf-37b2-4e88-bab4-2409de61701d",
   "metadata": {},
   "source": [
    "# Notebook 04 · Streaming Hourly Funnel Metrics by Brand\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook implements the **streaming analytics pipeline** for FunnelPulse. It consumes a simulated real-time event stream (built from historical bronze data) and continuously computes **hourly funnel metrics by brand**, writing the results as a streaming gold table.\n",
    "\n",
    "Conceptually, this is the **online counterpart** to the batch `gold_funnel_hourly_brand` table created in Notebook 01. The same business metrics are computed, but in a streaming fashion with windowing, watermarks, and checkpointing.\n",
    "\n",
    "---\n",
    "\n",
    "## Inputs and Outputs\n",
    "\n",
    "**Streaming source**\n",
    "\n",
    "- File based stream from `stream_input/`:\n",
    "  - Prepared in Notebook 03 by repartitioning a slice of `bronze_events`\n",
    "  - Contains many small Parquet files that simulate events arriving over time\n",
    "\n",
    "**Reference table**\n",
    "\n",
    "- `tables/bronze_events`\n",
    "  - Used to derive the schema for the streaming source\n",
    "\n",
    "**Streaming sink**\n",
    "\n",
    "- `tables/gold_stream_funnel_hourly_brand`\n",
    "  - Streaming gold table with hourly funnel metrics by brand\n",
    "  - Written incrementally as new events are processed\n",
    "  - Backed by a checkpoint directory for recovery\n",
    "\n",
    "---\n",
    "\n",
    "## High Level Workflow\n",
    "\n",
    "1. Initialize Spark and project paths\n",
    "2. Define a **streaming source** over `stream_input/` using the bronze schema\n",
    "3. Apply **streaming cleaning and normalization** (streaming “silver” stage)\n",
    "4. Apply **streaming aggregation** to compute hourly funnel metrics by brand\n",
    "5. Configure **watermarking** so completed windows can be emitted in append mode\n",
    "6. Start a **streaming query** that writes hourly brand metrics to a Parquet based streaming gold table\n",
    "7. Optionally monitor the query and then stop it after enough data has been processed\n",
    "\n",
    "---\n",
    "\n",
    "## Streaming Source Design\n",
    "\n",
    "Instead of connecting to Kafka directly, this notebook uses the log replay setup created in Notebook 03:\n",
    "\n",
    "- The streaming source is defined with `readStream` over the `stream_input/` directory\n",
    "- The schema is taken from `bronze_events` to ensure alignment with the batch pipeline\n",
    "- A `maxFilesPerTrigger` option controls how many new files are processed in each microbatch\n",
    "\n",
    "As Spark Structured Streaming runs, it treats each new Parquet file in `stream_input/` as the next batch of events in the stream.\n",
    "\n",
    "---\n",
    "\n",
    "## Streaming “Silver” · On-the-fly Cleaning and Normalization\n",
    "\n",
    "The first step on the streaming data is to apply the same kinds of transformations that the **silver** layer uses in batch:\n",
    "\n",
    "- Filter out events with:\n",
    "  - Missing or non positive prices\n",
    "  - Missing `event_time` or `event_type`\n",
    "- Normalize text fields:\n",
    "  - Lowercase brand into `brand_norm`\n",
    "  - Sanitize `category_code` into `category_code_norm`\n",
    "- Derive additional fields:\n",
    "  - `event_date` from `event_time`\n",
    "  - Data quality flags for missing session, brand, category\n",
    "\n",
    "These operations are applied to the streaming DataFrame, so every microbatch passes through the same quality and normalization logic as in the offline pipeline.\n",
    "\n",
    "The output of this step is a **streaming “cleaned events” view** that mirrors the batch silver table but is updated continuously.\n",
    "\n",
    "---\n",
    "\n",
    "## Streaming “Gold” · Hourly Funnel Metrics by Brand\n",
    "\n",
    "The core streaming computation reproduces the hourly funnel logic but on live data:\n",
    "\n",
    "**Grain**\n",
    "\n",
    "- One row per 1-hour window per brand:\n",
    "  - `window_start`, `window_end`\n",
    "  - `brand`\n",
    "\n",
    "**Metrics**\n",
    "\n",
    "- For each window × brand, the job maintains:\n",
    "  - `views`      count of view events\n",
    "  - `carts`      count of cart events\n",
    "  - `purchases`  count of purchase events\n",
    "  - `revenue`    sum of purchase prices\n",
    "- Derives funnel rates:\n",
    "  - `view_to_cart_rate`\n",
    "  - `cart_to_purchase_rate`\n",
    "  - `conversion_rate`\n",
    "- Adds `window_date` for partitioning\n",
    "\n",
    "This aggregation is defined using a time based window on `event_time` and a grouping key on `brand_norm`, just like in the batch pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## Watermarks and Output Mode\n",
    "\n",
    "Because the job is aggregating over time windows on a streaming source, it must decide when a window is **complete** and safe to emit in **append** mode.\n",
    "\n",
    "To achieve this, the notebook configures a **watermark** on `event_time`:\n",
    "\n",
    "- The watermark expresses the maximum allowed lateness (for example, 2 hours)\n",
    "- Spark keeps state for each window until the watermark passes the end of that window\n",
    "- Once a window is considered complete, its metrics are written to the sink and its state can be dropped\n",
    "\n",
    "With a watermark in place, the streaming query is allowed to use `outputMode(\"append\")`, which is appropriate for a production-like anomaly and alerting pipeline where completed windows are appended as new rows.\n",
    "\n",
    "---\n",
    "\n",
    "## Streaming Sink and Checkpointing\n",
    "\n",
    "The notebook configures a streaming sink that:\n",
    "\n",
    "- Writes results to `tables/gold_stream_funnel_hourly_brand` in Parquet format\n",
    "- Uses a dedicated `checkpointLocation` under `checkpoints/` for:\n",
    "  - Tracking offsets\n",
    "  - Maintaining aggregation state\n",
    "  - Enabling fault-tolerant recovery and exactly-once style semantics\n",
    "\n",
    "The query is started and runs until:\n",
    "\n",
    "- It has processed enough microbatches to demonstrate the pipeline, or\n",
    "- It is explicitly stopped in the notebook\n",
    "\n",
    "Even if the query is stopped early in this environment, the design and configuration mirror how a production streaming job would be deployed.\n",
    "\n",
    "---\n",
    "\n",
    "## Role of This Notebook in the Overall System\n",
    "\n",
    "This notebook is the core of the **online path** in FunnelPulse:\n",
    "\n",
    "- Notebooks 01 and 02:\n",
    "  - Build the **batch** lakehouse and gold tables for historical analysis\n",
    "- Notebook 03:\n",
    "  - Prepares a file based stream to emulate real-time traffic\n",
    "- Notebook 04 (this notebook):\n",
    "  - Implements Spark Structured Streaming logic that:\n",
    "    - Ingests the event stream\n",
    "    - Applies silver-like cleaning\n",
    "    - Computes hourly funnel metrics by brand in real time\n",
    "    - Writes a streaming gold table with checkpointing and watermarks\n",
    "- Notebook 05:\n",
    "  - Builds anomaly detection logic over the hourly funnel metrics\n",
    "\n",
    "Together, the batch and streaming pipelines show that FunnelPulse can support both **historical analysis** and **near real-time monitoring** of the e-commerce funnel at scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af400c57-fd9a-49f1-b158-c04a1e9cf059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mem:            15Gi       6.4Gi       6.6Gi        40Mi       3.0Gi       9.2Gi\n",
      "CPU(s):                                  2\n",
      "NUMA node0 CPU(s):                       0,1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/26 12:07:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/26 12:07:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/11/26 12:07:11 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "25/11/26 12:07:11 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x788428fb8380>\n",
      "Spark UI: https://csgy-6513-fall.rcnyu.org/user/ss18851/proxy/4040\n",
      "Bronze path          : /home/jovyan/funnelpulse/tables/bronze_events\n",
      "Silver path          : /home/jovyan/funnelpulse/tables/silver_events\n",
      "Stream input path    : /home/jovyan/funnelpulse/stream_input\n",
      "Stream checkpoint dir: /home/jovyan/funnelpulse/checkpoints/stream_hourly_brand\n",
      "Stream gold path     : /home/jovyan/funnelpulse/tables/gold_stream_funnel_hourly_brand\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: Spark init + paths for streaming job\n",
    "\n",
    "!free -h | grep \"Mem:\"\n",
    "!lscpu | grep \"CPU(s):\"\n",
    "\n",
    "import os\n",
    "import pyspark\n",
    "\n",
    "conf = pyspark.SparkConf()\n",
    "conf.set('spark.ui.proxyBase', '/user/' + os.environ['JUPYTERHUB_USER'] + '/proxy/4040')\n",
    "conf.set('spark.sql.repl.eagerEval.enabled', False)\n",
    "conf.set('spark.driver.memory','6g')\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf, master='local[*]')\n",
    "spark = pyspark.sql.SparkSession.builder.appName(\"FunnelPulse Streaming Hourly Brand\").getOrCreate()\n",
    "\n",
    "print(spark)\n",
    "print(f\"Spark UI: https://csgy-6513-fall.rcnyu.org{conf.get('spark.ui.proxyBase')}\")\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "project_root = os.path.join(home, \"funnelpulse\")\n",
    "tables_dir = os.path.join(project_root, \"tables\")\n",
    "\n",
    "bronze_path          = os.path.join(tables_dir, \"bronze_events\")\n",
    "silver_path          = os.path.join(tables_dir, \"silver_events\")  # we might reuse logic\n",
    "stream_input_path    = os.path.join(project_root, \"stream_input\")\n",
    "stream_checkpoint    = os.path.join(project_root, \"checkpoints\", \"stream_hourly_brand\")\n",
    "stream_gold_path     = os.path.join(tables_dir, \"gold_stream_funnel_hourly_brand\")\n",
    "\n",
    "print(\"Bronze path          :\", bronze_path)\n",
    "print(\"Silver path          :\", silver_path)\n",
    "print(\"Stream input path    :\", stream_input_path)\n",
    "print(\"Stream checkpoint dir:\", stream_checkpoint)\n",
    "print(\"Stream gold path     :\", stream_gold_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e43d1b1-9c25-454c-a591-29d873dc10a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze schema:\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      "\n",
      "Is stream_raw streaming? True\n",
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Define streaming source from stream_input (Parquet-based stream)\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Get schema from bronze (static read)\n",
    "bronze_sample = spark.read.parquet(bronze_path)\n",
    "bronze_schema = bronze_sample.schema\n",
    "\n",
    "print(\"Bronze schema:\")\n",
    "bronze_sample.printSchema()\n",
    "\n",
    "# Build file-based streaming source\n",
    "stream_raw = (\n",
    "    spark.readStream\n",
    "    .schema(bronze_schema)              # required for file-based streams\n",
    "    .option(\"maxFilesPerTrigger\", 1)    # process 1 new file per micro-batch\n",
    "    .parquet(stream_input_path)\n",
    ")\n",
    "\n",
    "print(\"Is stream_raw streaming?\", stream_raw.isStreaming)\n",
    "stream_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd184bbd-a03e-4c2c-a68e-2c76964f5b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_time: timestamp (nullable = true)\n",
      " |-- event_type: string (nullable = true)\n",
      " |-- product_id: integer (nullable = true)\n",
      " |-- category_id: long (nullable = true)\n",
      " |-- category_code: string (nullable = true)\n",
      " |-- brand: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- user_session: string (nullable = true)\n",
      " |-- event_date: date (nullable = true)\n",
      " |-- brand_norm: string (nullable = true)\n",
      " |-- category_code_norm: string (nullable = true)\n",
      " |-- dq_missing_session: boolean (nullable = false)\n",
      " |-- dq_missing_brand: boolean (nullable = false)\n",
      " |-- dq_missing_category: boolean (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Streaming cleaning (Silver-like) on stream_raw\n",
    "\n",
    "from pyspark.sql.functions import lower, regexp_replace, when, to_date\n",
    "\n",
    "stream_clean = stream_raw\n",
    "\n",
    "# Drop bad rows\n",
    "stream_clean = stream_clean.filter(stream_clean.price.isNotNull() & (stream_clean.price > 0))\n",
    "stream_clean = stream_clean.filter(stream_clean.event_time.isNotNull() & stream_clean.event_type.isNotNull())\n",
    "\n",
    "# Normalize brand/category\n",
    "stream_clean = (\n",
    "    stream_clean\n",
    "    .withColumn(\"brand_norm\", lower(col(\"brand\")))\n",
    "    .withColumn(\"category_code_norm\", lower(col(\"category_code\")))\n",
    "    .withColumn(\n",
    "        \"category_code_norm\",\n",
    "        regexp_replace(col(\"category_code_norm\"), \"[^a-z0-9\\\\.]\", \"_\")\n",
    "    )\n",
    "    .withColumn(\"event_date\", to_date(col(\"event_time\")))\n",
    "    .withColumn(\"dq_missing_session\", stream_clean.user_session.isNull())\n",
    "    .withColumn(\"dq_missing_brand\", stream_clean.brand.isNull())\n",
    "    .withColumn(\"dq_missing_category\", stream_clean.category_code.isNull())\n",
    ")\n",
    "\n",
    "stream_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3a722ef-1415-4d20-aed0-01982290063b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- brand: string (nullable = true)\n",
      " |-- views: long (nullable = true)\n",
      " |-- carts: long (nullable = true)\n",
      " |-- purchases: long (nullable = true)\n",
      " |-- revenue: double (nullable = true)\n",
      " |-- window_start: timestamp (nullable = true)\n",
      " |-- window_end: timestamp (nullable = true)\n",
      " |-- view_to_cart_rate: double (nullable = true)\n",
      " |-- cart_to_purchase_rate: double (nullable = true)\n",
      " |-- conversion_rate: double (nullable = true)\n",
      " |-- window_date: date (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, sum as _sum, to_date, col, when\n",
    "\n",
    "# CELL 4 (updated): Streaming aggregation - hourly funnel metrics by brand with watermark\n",
    "\n",
    "stream_windowed = (\n",
    "    stream_clean\n",
    "    # watermark says: we expect late events up to 2 hours behind the latest seen event_time\n",
    "    .withWatermark(\"event_time\", \"2 hours\")\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"1 hour\").alias(\"w\"),\n",
    "        col(\"brand_norm\").alias(\"brand\")\n",
    "    )\n",
    "    .agg(\n",
    "        _sum((col(\"event_type\") == \"view\").cast(\"int\")).alias(\"views\"),\n",
    "        _sum((col(\"event_type\") == \"cart\").cast(\"int\")).alias(\"carts\"),\n",
    "        _sum((col(\"event_type\") == \"purchase\").cast(\"int\")).alias(\"purchases\"),\n",
    "        _sum(\n",
    "            when(col(\"event_type\") == \"purchase\", col(\"price\")).otherwise(0.0)\n",
    "        ).alias(\"revenue\")\n",
    "    )\n",
    ")\n",
    "\n",
    "stream_gold = (\n",
    "    stream_windowed\n",
    "    .withColumn(\"window_start\", col(\"w.start\"))\n",
    "    .withColumn(\"window_end\", col(\"w.end\"))\n",
    "    .drop(\"w\")\n",
    ")\n",
    "\n",
    "stream_gold = (\n",
    "    stream_gold\n",
    "    .withColumn(\n",
    "        \"view_to_cart_rate\",\n",
    "        when(col(\"views\") > 0, col(\"carts\") / col(\"views\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"cart_to_purchase_rate\",\n",
    "        when(col(\"carts\") > 0, col(\"purchases\") / col(\"carts\"))\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"conversion_rate\",\n",
    "        when(col(\"views\") > 0, col(\"purchases\") / col(\"views\"))\n",
    "    )\n",
    "    .withColumn(\"window_date\", to_date(col(\"window_start\")))\n",
    ")\n",
    "\n",
    "stream_gold.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be7e4b93-e6bf-4a47-a056-bdf064f15de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 12:16:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming query started. ID: 9e3f0bd1-1f22-40f1-bdd4-ee889e46eab0\n",
      "Use 'stream_query.status' to check status.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=>                                                       (6 + 2) / 200]"
     ]
    }
   ],
   "source": [
    "# CELL 5: Start streaming query (write to Parquet and console)\n",
    "\n",
    "# Parquet sink\n",
    "stream_query = (\n",
    "    stream_gold\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")                # new windows only\n",
    "    .format(\"parquet\")\n",
    "    .option(\"checkpointLocation\", stream_checkpoint)\n",
    "    .option(\"path\", stream_gold_path)\n",
    "    .trigger(processingTime=\"10 seconds\")  # micro-batch every 10 seconds\n",
    "    .start()\n",
    ")\n",
    "\n",
    "print(\"Streaming query started. ID:\", stream_query.id)\n",
    "print(\"Use 'stream_query.status' to check status.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d2c2dca8-5e6c-4811-bcb3-c0d2fc30cf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status check 0 : {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:========>                                               (30 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status check 1 : {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:===============>                                        (57 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status check 2 : {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:========================>                               (86 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status check 3 : {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:================================>                      (119 + 2) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status check 4 : {'message': 'Processing new data', 'isDataAvailable': True, 'isTriggerActive': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 12:17:11 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 52059 milliseconds\n",
      "25/11/26 12:17:50 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 38364 milliseconds\n",
      "25/11/26 12:18:12 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 22445 milliseconds\n",
      "25/11/26 12:18:32 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 19545 milliseconds\n",
      "25/11/26 12:18:51 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 18917 milliseconds\n",
      "25/11/26 12:19:10 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 19340 milliseconds\n",
      "25/11/26 12:19:27 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 17246 milliseconds\n",
      "25/11/26 12:19:44 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 16447 milliseconds\n",
      "25/11/26 12:19:59 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 15824 milliseconds\n",
      "25/11/26 12:20:16 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 16248 milliseconds\n",
      "25/11/26 12:20:31 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 15626 milliseconds\n",
      "25/11/26 12:20:55 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 23928 milliseconds\n",
      "25/11/26 12:21:17 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 21620 milliseconds\n",
      "25/11/26 12:21:33 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 15708 milliseconds\n",
      "25/11/26 12:21:48 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 15395 milliseconds\n",
      "25/11/26 12:22:03 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 15058 milliseconds\n",
      "25/11/26 12:22:18 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 14908 milliseconds\n",
      "25/11/26 12:22:33 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 15271 milliseconds\n",
      "25/11/26 12:22:49 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 15520 milliseconds\n",
      "25/11/26 12:23:03 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 14586 milliseconds\n",
      "25/11/26 12:23:18 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 14798 milliseconds\n",
      "25/11/26 12:23:33 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 14617 milliseconds\n",
      "25/11/26 12:23:54 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 21641 milliseconds\n",
      "25/11/26 12:24:12 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 10000} milliseconds, but spent 17417 milliseconds\n",
      "[Stage 51:================================>                     (119 + 2) / 200]"
     ]
    }
   ],
   "source": [
    "# CELL 6: Monitor status a few times\n",
    "\n",
    "import time\n",
    "\n",
    "for i in range(5):\n",
    "    print(\"Status check\", i, \":\", stream_query.status)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6acd969-0bd6-4d62-b7f7-1a593e96f0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopped: 9e3f0bd1-1f22-40f1-bdd4-ee889e46eab0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 12:24:21 ERROR FileFormatWriter: Aborting job 17fb3c0d-c0fa-43f6-9d5e-ce14bac443e3.\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1048)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:243)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:255)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:104)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitReady(ThreadUtils.scala:374)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:998)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2484)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$1(FileFormatWriter.scala:240)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:270)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:211)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:192)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink.addBatch(FileStreamSink.scala:176)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:879)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$8(SQLExecution.scala:163)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSessionTagsApplied(SQLExecution.scala:272)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$7(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.$anonfun$withResources$1(ArtifactManager.scala:112)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withClassLoaderIfNeeded(ArtifactManager.scala:106)\n",
      "\tat org.apache.spark.sql.artifact.ArtifactManager.withResources(ArtifactManager.scala:111)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:295)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId0$1(SQLExecution.scala:124)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId0(SQLExecution.scala:78)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:237)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:876)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$executeOneBatch$2(MicroBatchExecution.scala:394)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressContext.reportTimeTaken(ProgressReporter.scala:186)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.executeOneBatch(MicroBatchExecution.scala:364)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1$adapted(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch(TriggerExecutor.scala:39)\n",
      "\tat org.apache.spark.sql.execution.streaming.TriggerExecutor.runOneBatch$(TriggerExecutor.scala:37)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.runOneBatch(TriggerExecutor.scala:70)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:82)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:344)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:337)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:311)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:226)\n",
      "25/11/26 12:24:22 WARN Shell: Interrupted while joining on: Thread[Thread-54757,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1313)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1381)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1103)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1063)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:959)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1282)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1377)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1359)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:225)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1251)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1240)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1211)\n",
      "\tat org.apache.hadoop.fs.FileSystem.rename(FileSystem.java:1673)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.renameInternal(DelegateToFileSystem.java:211)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:896)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:807)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:512)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:807)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1044)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:376)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:156)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:447)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:174)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$2(statefulOperators.scala:797)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:671)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:797)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:390)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$2(FileFormatWriter.scala:251)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/26 12:24:22 WARN Shell: Interrupted while joining on: Thread[Thread-54756,5,]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1313)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1381)\n",
      "\tat org.apache.hadoop.util.Shell.joinThread(Shell.java:1103)\n",
      "\tat org.apache.hadoop.util.Shell.runCommand(Shell.java:1063)\n",
      "\tat org.apache.hadoop.util.Shell.run(Shell.java:959)\n",
      "\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:1282)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1377)\n",
      "\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:1359)\n",
      "\tat org.apache.hadoop.fs.FileUtil.readLink(FileUtil.java:225)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileLinkStatusInternal(RawLocalFileSystem.java:1251)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:1240)\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatus(RawLocalFileSystem.java:1211)\n",
      "\tat org.apache.hadoop.fs.DelegateToFileSystem.getFileLinkStatus(DelegateToFileSystem.java:138)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.renameInternal(AbstractFileSystem.java:857)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:807)\n",
      "\tat org.apache.hadoop.fs.ChecksumFs.renameInternal(ChecksumFs.java:519)\n",
      "\tat org.apache.hadoop.fs.AbstractFileSystem.rename(AbstractFileSystem.java:807)\n",
      "\tat org.apache.hadoop.fs.FileContext.rename(FileContext.java:1044)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileContextBasedCheckpointFileManager.renameTempFile(CheckpointFileManager.scala:376)\n",
      "\tat org.apache.spark.sql.execution.streaming.CheckpointFileManager$RenameBasedFSDataOutputStream.close(CheckpointFileManager.scala:156)\n",
      "\tat net.jpountz.lz4.LZ4BlockOutputStream.close(LZ4BlockOutputStream.java:198)\n",
      "\tat java.base/java.io.FilterOutputStream.close(FilterOutputStream.java:188)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.finalizeDeltaFile(HDFSBackedStateStoreProvider.scala:598)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider.org$apache$spark$sql$execution$streaming$state$HDFSBackedStateStoreProvider$$commitUpdates(HDFSBackedStateStoreProvider.scala:447)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.HDFSBackedStateStoreProvider$HDFSBackedStateStore.commit(HDFSBackedStateStoreProvider.scala:174)\n",
      "\tat org.apache.spark.sql.execution.streaming.state.StreamingAggregationStateManagerBaseImpl.commit(StreamingAggregationStateManager.scala:90)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.$anonfun$close$2(statefulOperators.scala:797)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.timeTakenMs(Utils.scala:481)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs(statefulOperators.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreWriter.timeTakenMs$(statefulOperators.scala:348)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec.timeTakenMs(statefulOperators.scala:671)\n",
      "\tat org.apache.spark.sql.execution.streaming.StateStoreSaveExec$$anon$2.close(statefulOperators.scala:797)\n",
      "\tat org.apache.spark.util.NextIterator.closeIfNeeded(NextIterator.scala:66)\n",
      "\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:75)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage5.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:50)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:390)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1323)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:418)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$2(FileFormatWriter.scala:251)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:171)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:147)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$5(Executor.scala:647)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:80)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:77)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:99)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:650)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "25/11/26 12:24:22 WARN DAGScheduler: Failed to cancel job group 2bc39b8c-2355-4541-9218-1669d403094e. Cannot find active jobs for it.\n",
      "25/11/26 12:24:22 WARN TaskSetManager: Lost task 121.0 in stage 51.0 (TID 5009) (jupyter-ss18851 executor driver): TaskKilled (Stage cancelled: [SPARK_JOB_CANCELLED] Job 26 cancelled Query [id = 9e3f0bd1-1f22-40f1-bdd4-ee889e46eab0, runId = 2bc39b8c-2355-4541-9218-1669d403094e] was stopped SQLSTATE: XXKDA)\n",
      "25/11/26 12:24:22 WARN TaskSetManager: Lost task 122.0 in stage 51.0 (TID 5010) (jupyter-ss18851 executor driver): TaskKilled (Stage cancelled: [SPARK_JOB_CANCELLED] Job 26 cancelled Query [id = 9e3f0bd1-1f22-40f1-bdd4-ee889e46eab0, runId = 2bc39b8c-2355-4541-9218-1669d403094e] was stopped SQLSTATE: XXKDA)\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Stop streaming query\n",
    "\n",
    "stream_query.stop()\n",
    "print(\"Stopped:\", stream_query.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd6b69f3-1d29-4548-bdae-63b7e708150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows in streaming gold funnel table: 17027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+---------+-------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "|brand     |views|carts|purchases|revenue|window_start       |window_end         |view_to_cart_rate  |cart_to_purchase_rate|conversion_rate    |window_date|\n",
      "+----------+-----+-----+---------+-------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "|NULL      |38   |15   |1        |3.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.39473684210526316|0.06666666666666667  |0.02631578947368421|2019-10-15 |\n",
      "|airnails  |1    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|1.0                |0.0                  |0.0                |2019-10-15 |\n",
      "|art-visage|1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|artex     |1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|balbcare  |0    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|bioaqua   |1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|bpw.style |1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|cnd       |1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|concept   |0    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|cosmoprofi|1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|domix     |0    |2    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|estel     |0    |2    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|farmstay  |0    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|grattol   |3    |0    |1        |2.3    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.3333333333333333 |2019-10-15 |\n",
      "|haruyama  |3    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.3333333333333333 |0.0                  |0.0                |2019-10-15 |\n",
      "|ingarden  |2    |2    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|1.0                |0.0                  |0.0                |2019-10-15 |\n",
      "|irisk     |1    |2    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|2.0                |0.0                  |0.0                |2019-10-15 |\n",
      "|italwax   |2    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.5                |0.0                  |0.0                |2019-10-15 |\n",
      "|jessnail  |2    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|kapous    |1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "+----------+-----+-----+---------+-------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch gold rows: 183145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 62:=============================>                            (2 + 2) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----+---------+-----------------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "|brand      |views|carts|purchases|revenue          |window_start       |window_end         |view_to_cart_rate  |cart_to_purchase_rate|conversion_rate    |window_date|\n",
      "+-----------+-----+-----+---------+-----------------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "|NULL       |202  |185  |59       |284.0199999999999|2019-09-30 20:00:00|2019-09-30 21:00:00|0.9158415841584159 |0.31891891891891894  |0.29207920792079206|2019-09-30 |\n",
      "|airnails   |0    |2    |2        |2.38             |2019-09-30 20:00:00|2019-09-30 21:00:00|NULL               |1.0                  |NULL               |2019-09-30 |\n",
      "|ardell     |1    |0    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|0.0                |NULL                 |0.0                |2019-09-30 |\n",
      "|art-visage |2    |4    |2        |5.640000000000001|2019-09-30 20:00:00|2019-09-30 21:00:00|2.0                |0.5                  |1.0                |2019-09-30 |\n",
      "|beautix    |2    |2    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|1.0                |0.0                  |0.0                |2019-09-30 |\n",
      "|beauty-free|3    |0    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|0.0                |NULL                 |0.0                |2019-09-30 |\n",
      "|bioaqua    |1    |1    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|1.0                |0.0                  |0.0                |2019-09-30 |\n",
      "|bluesky    |7    |4    |19       |81.77999999999999|2019-09-30 20:00:00|2019-09-30 21:00:00|0.5714285714285714 |4.75                 |2.7142857142857144 |2019-09-30 |\n",
      "|bpw.style  |3    |4    |6        |5.22             |2019-09-30 20:00:00|2019-09-30 21:00:00|1.3333333333333333 |1.5                  |2.0                |2019-09-30 |\n",
      "|browxenna  |2    |0    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|0.0                |NULL                 |0.0                |2019-09-30 |\n",
      "|cnd        |15   |5    |1        |15.71            |2019-09-30 20:00:00|2019-09-30 21:00:00|0.3333333333333333 |0.2                  |0.06666666666666667|2019-09-30 |\n",
      "|concept    |11   |5    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|0.45454545454545453|0.0                  |0.0                |2019-09-30 |\n",
      "|cosmoprofi |3    |4    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|1.3333333333333333 |0.0                  |0.0                |2019-09-30 |\n",
      "|cutrin     |0    |0    |1        |6.54             |2019-09-30 20:00:00|2019-09-30 21:00:00|NULL               |NULL                 |NULL               |2019-09-30 |\n",
      "|de.lux     |2    |1    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|0.5                |0.0                  |0.0                |2019-09-30 |\n",
      "|depilflax  |5    |0    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|0.0                |NULL                 |0.0                |2019-09-30 |\n",
      "|dizao      |1    |2    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|2.0                |0.0                  |0.0                |2019-09-30 |\n",
      "|domix      |2    |1    |1        |2.68             |2019-09-30 20:00:00|2019-09-30 21:00:00|0.5                |1.0                  |0.5                |2019-09-30 |\n",
      "|emil       |0    |1    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|NULL               |0.0                  |NULL               |2019-09-30 |\n",
      "|enas       |1    |0    |0        |0.0              |2019-09-30 20:00:00|2019-09-30 21:00:00|0.0                |NULL                 |0.0                |2019-09-30 |\n",
      "+-----------+-----+-----+---------+-----------------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# In the same notebook or a new one\n",
    "\n",
    "from pyspark.sql.functions import desc\n",
    "\n",
    "stream_gold_df = spark.read.parquet(stream_gold_path)\n",
    "\n",
    "print(\"Rows in streaming gold funnel table:\", stream_gold_df.count())\n",
    "stream_gold_df.orderBy(\"window_start\", \"brand\").show(20, truncate=False)\n",
    "\n",
    "# Compare with your batch gold table if you like:\n",
    "batch_gold_df = spark.read.parquet(os.path.join(tables_dir, \"gold_funnel_hourly_brand\"))\n",
    "\n",
    "print(\"Batch gold rows:\", batch_gold_df.count())\n",
    "batch_gold_df.orderBy(\"window_start\", \"brand\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03fce399-110d-4008-a40a-e3b9822d8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch subset rows: 49736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream rows      : 17027\n",
      "+-----------+-----------+---------------+----------------+\n",
      "|views_batch|carts_batch|purchases_batch|   revenue_batch|\n",
      "+-----------+-----------+---------------+----------------+\n",
      "|     994940|     586150|         135298|659827.479999999|\n",
      "+-----------+-----------+---------------+----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 75:=================================>                        (4 + 2) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+----------------+------------------+\n",
      "|views_stream|carts_stream|purchases_stream|    revenue_stream|\n",
      "+------------+------------+----------------+------------------+\n",
      "|       40104|       23636|            5246|25270.739999999976|\n",
      "+------------+------------+----------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum as _sum\n",
    "\n",
    "# Read both tables\n",
    "batch_gold = spark.read.parquet(os.path.join(tables_dir, \"gold_funnel_hourly_brand\"))\n",
    "stream_gold_df = spark.read.parquet(stream_gold_path)\n",
    "\n",
    "# Filter batch to the streaming date range (2019-10-15 to 2019-10-31)\n",
    "batch_subset = batch_gold.filter(\n",
    "    (col(\"window_date\") >= \"2019-10-15\") & (col(\"window_date\") <= \"2019-10-31\")\n",
    ")\n",
    "\n",
    "print(\"Batch subset rows:\", batch_subset.count())\n",
    "print(\"Stream rows      :\", stream_gold_df.count())\n",
    "\n",
    "# Compare aggregate metrics over that range (should be very close / identical)\n",
    "batch_summary = batch_subset.agg(\n",
    "    _sum(\"views\").alias(\"views_batch\"),\n",
    "    _sum(\"carts\").alias(\"carts_batch\"),\n",
    "    _sum(\"purchases\").alias(\"purchases_batch\"),\n",
    "    _sum(\"revenue\").alias(\"revenue_batch\")\n",
    ")\n",
    "stream_summary = stream_gold_df.agg(\n",
    "    _sum(\"views\").alias(\"views_stream\"),\n",
    "    _sum(\"carts\").alias(\"carts_stream\"),\n",
    "    _sum(\"purchases\").alias(\"purchases_stream\"),\n",
    "    _sum(\"revenue\").alias(\"revenue_stream\")\n",
    ")\n",
    "\n",
    "batch_summary.show()\n",
    "stream_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8137884c-a350-4f94-8f34-864e194f276b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATCH:\n",
      "+-----------+-----+-----+---------+------------------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "|brand      |views|carts|purchases|revenue           |window_start       |window_end         |view_to_cart_rate  |cart_to_purchase_rate|conversion_rate    |window_date|\n",
      "+-----------+-----+-----+---------+------------------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "|NULL       |562  |414  |22       |119.00999999999999|2019-10-15 00:00:00|2019-10-15 01:00:00|0.7366548042704626 |0.05314009661835749  |0.03914590747330961|2019-10-15 |\n",
      "|airnails   |4    |15   |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|3.75               |0.0                  |0.0                |2019-10-15 |\n",
      "|ardell     |2    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|art-visage |3    |1    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.3333333333333333 |0.0                  |0.0                |2019-10-15 |\n",
      "|artex      |4    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|australis  |1    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|balbcare   |0    |6    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|beautix    |3    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|beauty-free|1    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|bespecial  |1    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|bioaqua    |4    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|bluesky    |7    |5    |3        |11.91             |2019-10-15 00:00:00|2019-10-15 01:00:00|0.7142857142857143 |0.6                  |0.42857142857142855|2019-10-15 |\n",
      "|bodyton    |2    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|bpw.style  |49   |12   |1        |1.59              |2019-10-15 00:00:00|2019-10-15 01:00:00|0.24489795918367346|0.08333333333333333  |0.02040816326530612|2019-10-15 |\n",
      "|browxenna  |8    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|carmex     |1    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|chi        |0    |1    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|cnd        |21   |4    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.19047619047619047|0.0                  |0.0                |2019-10-15 |\n",
      "|concept    |31   |5    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.16129032258064516|0.0                  |0.0                |2019-10-15 |\n",
      "|coocla     |1    |0    |0        |0.0               |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "+-----------+-----+-----+---------+------------------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "only showing top 20 rows\n",
      "STREAM:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 79:=================================>                        (4 + 2) / 7]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+-----+---------+-------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "|brand     |views|carts|purchases|revenue|window_start       |window_end         |view_to_cart_rate  |cart_to_purchase_rate|conversion_rate    |window_date|\n",
      "+----------+-----+-----+---------+-------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "|NULL      |38   |15   |1        |3.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.39473684210526316|0.06666666666666667  |0.02631578947368421|2019-10-15 |\n",
      "|airnails  |1    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|1.0                |0.0                  |0.0                |2019-10-15 |\n",
      "|art-visage|1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|artex     |1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|balbcare  |0    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|bioaqua   |1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|bpw.style |1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|cnd       |1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|concept   |0    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|cosmoprofi|1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|domix     |0    |2    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|estel     |0    |2    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|farmstay  |0    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|NULL               |0.0                  |NULL               |2019-10-15 |\n",
      "|grattol   |3    |0    |1        |2.3    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.3333333333333333 |2019-10-15 |\n",
      "|haruyama  |3    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.3333333333333333 |0.0                  |0.0                |2019-10-15 |\n",
      "|ingarden  |2    |2    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|1.0                |0.0                  |0.0                |2019-10-15 |\n",
      "|irisk     |1    |2    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|2.0                |0.0                  |0.0                |2019-10-15 |\n",
      "|italwax   |2    |1    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.5                |0.0                  |0.0                |2019-10-15 |\n",
      "|jessnail  |2    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "|kapous    |1    |0    |0        |0.0    |2019-10-15 00:00:00|2019-10-15 01:00:00|0.0                |NULL                 |0.0                |2019-10-15 |\n",
      "+----------+-----+-----+---------+-------+-------------------+-------------------+-------------------+---------------------+-------------------+-----------+\n",
      "only showing top 20 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "test_hour = \"2019-10-15 00:00:00\"\n",
    "\n",
    "batch_hour_brand = batch_subset.filter(col(\"window_start\") == test_hour)\n",
    "stream_hour_brand = stream_gold_df.filter(col(\"window_start\") == test_hour)\n",
    "\n",
    "print(\"BATCH:\")\n",
    "batch_hour_brand.orderBy(\"brand\").show(20, truncate=False)\n",
    "\n",
    "print(\"STREAM:\")\n",
    "stream_hour_brand.orderBy(\"brand\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d18a5e-2905-4598-ae76-b4032d9bee5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bigdata]",
   "language": "python",
   "name": "conda-env-bigdata-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
